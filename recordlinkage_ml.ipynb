{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#      Python Libraries \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import recordlinkage as rl\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "\n",
    "# Data Preprocessing and loading\n",
    "\n",
    "pd.set_option('display.max_rows', 1200)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "# Data Loading\n",
    "\n",
    "parse_dates = ['DataSides.0.Settle Date', 'DataSides.1.Settle Date','DataSides.0.Trade Date','DataSides.1.Trade Date']\n",
    "data = pd.read_excel(\"/home/anshul/record_linkage_api/Reconcilliation Data Mastering Using AI.xlsx\",parse_dates=parse_dates, header=1)\n",
    "\n",
    "\n",
    "m = pd.DataFrame({'pa': range(1, len(data) + 1, 1)})\n",
    "data_source_0 = data.iloc[:,:46]\n",
    "data_source_0 = data_source_0.join(m)\n",
    "data_source_1 = data.iloc[:, 46:92]\n",
    "data_source_1 = data_source_1.join(m)\n",
    "\n",
    "\n",
    "# making new feature(Column) by adding cancel amount and net amount column\n",
    "\n",
    "data_source_0['DataSides.0.Cancel Amount'] = data_source_0['DataSides.0.Cancel Amount'].replace(np.nan, 0)\n",
    "data_source_0['Cancel_and_net_amount_addition'] = data_source_0['DataSides.0.Cancel Amount'] + data_source_0['DataSides.0.Net Amount']\n",
    "data_source_1['DataSides.1.Cancel Amount'] = data_source_1['DataSides.1.Cancel Amount'].replace(np.nan, 0)\n",
    "data_source_1['Cancel_and_net_amount_addition'] = data_source_1['DataSides.1.Cancel Amount'] + data_source_1['DataSides.1.Net Amount']\n",
    "\n",
    "# making new feature(Column) by difference of settle date and trade date\n",
    "\n",
    "data_source_0['diff_date'] = data_source_0['DataSides.0.Settle Date'].sub(data_source_0['DataSides.0.Trade Date'], axis=0)\n",
    "data_source_0['diff_date'] = data_source_0['diff_date'] / np.timedelta64(1, 'D')\n",
    "data_source_1['diff_date'] = data_source_1['DataSides.1.Settle Date'].sub(data_source_1['DataSides.1.Trade Date'], axis=0)\n",
    "data_source_1['diff_date'] = data_source_1['diff_date'] / np.timedelta64(1, 'D')\n",
    "\n",
    "# Preparing Data for training and testing \n",
    "\n",
    "#\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\     for true label          \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\    \n",
    "#\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\                              \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\   \n",
    "\n",
    "source_0 = data_source_0\n",
    "source_1 = data_source_1\n",
    "pcl=rl.BlockIndex(on='pa')                     # Block Indexing \n",
    "pairs = pcl.index(data_source_0,data_source_1)\n",
    "true_index = pairs                              # true matching record pairs\n",
    "\n",
    "#\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\     for false label         \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\    \n",
    "#\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\                              \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\  \n",
    "\n",
    "data_A = data_source_0[:900]        #Selecting the top 300 records\n",
    "data_B = data_source_1[:900]\n",
    "\n",
    "# data_A = data_source_0[758:760]       #Selecting the top 300 records\n",
    "# data_B = data_source_1[758:760]\n",
    "\n",
    "\n",
    "# data_A['DataSides.0.Transaction Type'] = data_A['DataSides.0.Transaction Type'].str.encode('utf-8')\n",
    "# data_B['DataSides.1.Transaction Type'] = data_B['DataSides.1.Transaction Type'].str.encode('utf-8')\n",
    "\n",
    "pcl=rl.FullIndex()                 # Full Indexing for the false matching records\n",
    "pairs = pcl.index(data_A,data_B)   # multi index pairs \n",
    "\n",
    "#\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\     Record linkage implementation for matching score  \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\    \n",
    "#\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\                                                      \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ \n",
    "\n",
    "compare_cl = rl.Compare(pairs, data_A, data_B) # compare vector  \n",
    "\n",
    "# compairing score on exact matching\n",
    "\n",
    "compare_cl.exact('DataSides.0.Currency', 'DataSides.1.Currency', name='Currency',missing_value=np.nan)   \n",
    "compare_cl.exact('DataSides.0.Mapped Custodian Account', 'DataSides.1.Mapped Custodian Account', name='Mapped Custodian',missing_value=np.nan)\n",
    "compare_cl.exact('DataSides.0.SEDOL', 'DataSides.1.SEDOL', name='SEDOL',missing_value=1)\n",
    "compare_cl.exact('DataSides.0.CUSIP', 'DataSides.1.CUSIP', name='CUSIP',missing_value=1)\n",
    "compare_cl.exact('DataSides.0.ISIN', 'DataSides.1.ISIN', name='ISIN',missing_value=1)\n",
    "compare_cl.exact('DataSides.0.Underlying ISIN', 'DataSides.1.Underlying ISIN', name='ISIN_u',missing_value=1)\n",
    "compare_cl.exact('DataSides.0.Underlying Sedol', 'DataSides.1.Underlying Sedol', name='SEDOL_u',missing_value=1)\n",
    "compare_cl.exact('DataSides.0.Underlying Cusip', 'DataSides.1.Underlying Cusip', name='CUSIP_u',missing_value=1)\n",
    "compare_cl.exact('DataSides.0.Cancel Amount', 'DataSides.1.Cancel Amount', name='Cancel Amount',missing_value=np.nan)\n",
    "compare_cl.exact('Cancel_and_net_amount_addition', 'Cancel_and_net_amount_addition', name='add_amount',missing_value=np.nan)\n",
    "compare_cl.exact('diff_date', 'diff_date', name='diff_date',missing_value=np.nan)\n",
    "\n",
    "# compairing score on date matching\n",
    "\n",
    "compare_cl.date('DataSides.0.Trade Date', 'DataSides.1.Trade Date', name='Trade Date',missing_value=np.nan)\n",
    "compare_cl.date('DataSides.0.Settle Date', 'DataSides.1.Settle Date', name='Settle Date',missing_value=np.nan)\n",
    "compare_cl.date('DataSides.0.Business Date', 'DataSides.1.Business Date', name='Business Date',missing_value=np.nan)\n",
    "\n",
    "# compairing score on string matching\n",
    "\n",
    "compare_cl.string('DataSides.0.Transaction Type', 'DataSides.1.Transaction Type',method='jarowinkler', name='Transaction Type',missing_value=np.nan)\n",
    "compare_cl.string('DataSides.0.Investment Type', 'DataSides.1.Investment Type', method='jarowinkler', name='Investment type',missing_value=np.nan)\n",
    "\n",
    "\n",
    "\n",
    "compare_score_data = compare_cl.vectors     # dataframe of matching score\n",
    "compare_score_data = compare_score_data.fillna(method='ffill')   # null treatment\n",
    "\n",
    "# making of true and false status record\n",
    "\n",
    "true_label_data = compare_score_data.loc[(compare_score_data.index.get_level_values(1) ) == (compare_score_data.index.get_level_values(0) )]  #filtering match records         \n",
    "false_label_data_cross = compare_score_data.loc[(compare_score_data.index.get_level_values(1) ) != (compare_score_data.index.get_level_values(0) )]  #filtering not match records\n",
    "false_label_data = false_label_data_cross.sample(frac=0.0011)  # selecting records from cross joined records\n",
    "true_label_data['Actual_match_status'] = True     # labeling for the true records\n",
    "false_label_data['Actual_match_status'] = False    # labeling for the false records\n",
    "\n",
    "final_labeled_data = true_label_data.append(false_label_data) # the final datafrme with both true and false labeled records\n",
    "final_labeled_data_index = final_labeled_data.index    # index for the final dataset \n",
    "\n",
    "\n",
    "# splitting data in 70 and 30 percent ratio\n",
    "\n",
    "msk = np.random.rand(len(final_labeled_data)) < 0.70   # randomly splitting data\n",
    "\n",
    "train = final_labeled_data[msk]  # trainig data\n",
    "test = final_labeled_data[~msk]  # testing data\n",
    "\n",
    "train_X = train.iloc[:,0:16]     # trainig data input columns\n",
    "train_Y = train.iloc[:,16:17]    # trainig data output column\n",
    "\n",
    "test_X = test.iloc[:,0:16]       # testing data input columns\n",
    "test_Y = test.iloc[:,16:17]      # testing data output column\n",
    "\n",
    "# Model training\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(10, ))   # multilayer perceptron classifier with the 10 hidden layer\n",
    "clf.fit(train_X,train_Y)                         # Training model for prediction\n",
    "\n",
    "# Prediction and result\n",
    "\n",
    "predicted = clf.predict(test_X)  # prediction on the testing data\n",
    "print(\"Accuracy score :\", accuracy_score(test_Y, predicted))   # Accuracy of the prediction\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(test_Y, predicted).ravel()\n",
    "print( \"True negative:= \",tn)\n",
    "print (\"false positive:= \",fp)\n",
    "print (\"false negative:= \",fn)\n",
    "print (\"True positive:= \",tp)\n",
    "\n",
    "# Plots\n",
    "\n",
    "# Record In testing dataset  \n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "\n",
    "objects = ('True', 'False')\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [(fn+tp),(tn+fp)]\n",
    " \n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "for a,b in zip(y_pos,performance):\n",
    "    plt.text(a, b, str(b))\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('Number of Record')\n",
    "plt.title('True and False record in testing dataset')\n",
    " \n",
    "plt.show()\n",
    "\n",
    "# Predicted True and False record by model\n",
    "\n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "\n",
    "objects = ('True', 'False')\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [(tp+fp),(tn+fn)]\n",
    " \n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "for a,b in zip(y_pos,performance):\n",
    "    plt.text(a, b, str(b))\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('Number of Record')\n",
    "plt.title('Predicted True and False record by model')\n",
    " \n",
    "plt.show()\n",
    "\n",
    "# Summary of prediction\n",
    "\n",
    "\n",
    "objects = ('True record and predicted True', 'False record and predicted true ','False record and predicted false ','True record and predicted False')\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [tp,fp,tn,fn]\n",
    " \n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "for a,b in zip(y_pos,performance):\n",
    "    plt.text(a, b, str(b))\n",
    "plt.xticks(y_pos, objects,rotation=70)\n",
    "plt.ylabel('Number of Record')\n",
    "plt.title('Summary of prediction')\n",
    " \n",
    "plt.show()\n",
    "\n",
    "# Saving Prediction with match status\n",
    "\n",
    "test_index = test_X.index.values.tolist()\n",
    "match_status = pd.DataFrame(predicted,columns=['Pridicted match status'])\n",
    "\n",
    "list_source_0 = [i[0] for i in test_index]\n",
    "list_source_1 = [i[1] for i in test_index]\n",
    "\n",
    "data_0 = data_source_0.loc[list_source_0]\n",
    "data_1 = data_source_1.loc[list_source_1]\n",
    "\n",
    "data_0.reset_index(inplace=True)\n",
    "data_1.reset_index(inplace=True)\n",
    "test_Y.reset_index(inplace=True)\n",
    "\n",
    "result = pd.concat([data_0, data_1,match_status,test_Y[['Actual_match_status']]],axis=1)\n",
    "\n",
    "# encoding=utf8\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "result.to_csv('final.csv')\n",
    "train.to_csv('train.csv')\n",
    "test.to_csv('test.csv')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
